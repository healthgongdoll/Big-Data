# Big-Data

Big Data Studies Note

## Frequent Itemset Mining & Association Rules 

Items - elements in basket 
Basket - small subset of items (transactions) 

![image](https://user-images.githubusercontent.com/79100627/195438680-a485938c-db34-444d-b799-fa0d6c0d6a63.png)

{milk} -> {coke}: If someone buy milk, he is most likely buy a coke

### Example of Baskets and Items 

Basket: Sentence Items: Words 
Basket: Patient Items: Side Effects 

Support for Itemset I: Number of baskets containing all items in I. 
(Frequent Itemset) Support Threshold: Appear at least s baskets (Transactions) 

Assoication Rule: {i1,i2,i3 ... ik} -> j

if basket contains i1 ~ ik then it is likely to contain j

## Confidence 

Confidence (i -> j) = Support(I U J) / Support (I)

*Not all the high confidence rules are interesting. For example, if everyone bought coke this might be useless information

## Interest 

Interest (i -> j) = Confidence( i -> j) - Prob[j]

## Example 1

![image](https://user-images.githubusercontent.com/79100627/195440190-51a73a97-0a2e-4f01-8e9a-dbba6274ff0a.png)

What is the confidence of {m,b} -> c?

Support for {m,b,c} = 2 
Support for {m,b} = 4

Since confidence = Support (i U j) / support(i) = 2/4 = 1/2

What could be the interst?

Conf(i - j) = 1/2 and probability of {c} is 5/8. 
Int(i-> j) = 1/2 - 5/8  

If {i1,i2,...,ik} -> j has high support and high confidence then {i1,i2,...ik} and {i1,i2,...ik} also "frequent" 

## Mining Assoication Rule 

Step 1. Find all frequent itemset I.
Step 2. Rule generation : If I is frequent, A is also frequent 

![image](https://user-images.githubusercontent.com/79100627/195441070-42653621-1a8f-4810-af49-777ef4fe8c58.png)

Items sets that are above the threshold <br/>
Items: m, c, b, j <br/>
Pair item candidate N*(N-1)/2 = {m,c}, {m,b}, {m,j}, {c,b}, {c,j}, {b,j}<br/>
Pair item counts = {m,c,3}, {m,b,4}, {m,j,2}, {c,b,5}, {c,j,3}, {b,j,2}<br/>
Confidence of frequent pairs <br/>
Conf(m->c) = Support(m U c) / support(m) = 3/5<br/>
Conf(m->b) = Support(m U b) / support(m) = 4/5<br/>
Conf(m->j) = Support(m U j) / support(m) = 2/5<br/>
Conf(c->b) = Support (c U b) / support(c) = 5/5 <br/>
Conf(c->j) = Support (c U j) / support(c) = 3/5<br/>
Conf(b->j) = Support (b U j) / support(b) = 2/6<br/>

overall, m->b, m->c, b->c, c->j 

## Maximal Frequent Itemset -> s = 3 (Only count the Superset) 

A - 4  (NO)<br/>
B - 5 (NO)<br/>
C - 3 (NO)<br/>
AB - 4 (Yes)<br/>
AC - 2 x<br/>
BC - 3 (Yes)<br/>
ABC - 2 x<br/>


## Counting Pairs
The Hardest part is **Computing Pairs**.

There are two approahces counting Pairs 

### Approach 1: 4 bytes Pairs  (Matrix)

![image](https://user-images.githubusercontent.com/79100627/195444181-05f9afed-b284-444d-ae64-3e7b5de9b67d.png)

### Approach 2: 12 bytes Pairs (Triples) (Only Count c > 0) 

![image](https://user-images.githubusercontent.com/79100627/195444297-93525458-1168-4e31-a573-27980fb0a6d1.png)

Approach 1 < Approach 2 iff less than 1/3 pairs occur) 

## Apriori Algorithm k pass 

Key Idea is monoticity 

If a set of items i appears at least s times, so does every subset of i 

![image](https://user-images.githubusercontent.com/79100627/195444727-200896bf-7103-4db9-81bb-86fe1d131b2f.png)

![image](https://user-images.githubusercontent.com/79100627/195444776-6e4435e4-6edd-49fb-8cb5-3f222b37b14a.png)

*C3 is generated by L1 & C2 (make candidates of triples with l1 and c2) 

## PCY algorithm (Park-Chen-Yu) Algorithm

In Apriori algorithm, during the pass1, most memory is idle
- Count the individual item 
- each pair of item, hash the pari to the bucket 

Pass2 
- Only count pairs that hashed to the bucket. 

![image](https://user-images.githubusercontent.com/79100627/195446313-312d76a5-d9ea-4150-91e4-a30ccd3059d1.png)

Another approach is to replace buckets by a bit-vector:
- 1 means the bucket count exceeded the support s (call it a frequent bucket); 0 means it did not 

## Multistage Algorithm 

Limit the number of candidate to be counted 
- For Big amount of data, the Main memory is the bottleneck 
- Still need to generate all itemsets but we only want to count/ keep track of that are frequent 

After Pass 1 of PCY, rehash only those pairs that qualify for pass 2 

Require three passes 

## Multistage vs Multihash 

Multistage 
- 3 Passes
- Bit-Vector eventually consume all main memory 

Multihash 
- Use Several Hashtables on first stage 
- Having number of buckets doubles average count 
- 2 Passes 

## Frequent Itemset in <= 2 passes

- Random Sampling
  - Take Random sample (Portion of Basket) 
  - Smaller Threshold 
  - Itemset become a candidate if it is found to be frequent 
- SON 
  - distributed data mining 
  - Compute freuqent item for each node 
- Toivonen 

![image](https://user-images.githubusercontent.com/79100627/195673540-c754f884-8f26-4e56-9b06-b9f3ebf09cec.png)

First of all, we have to find a frequent items before we find the frequent pairs 

Frequent Items are Cat, And, Dog, A, Training 

Generated candidate pairs (cat,and), (cat,dog), (cat, a) (cat, training), (and,dog),(and,a), (and, training), (dog,a), (dog, training), (a,training) 

Frequent Pairs are (Cat,and), (cat dog), (cat a), (and, dog), (dog,a) 

What is the confidence ((cat,dog) -> and)?

Confidence = Support(i U j) / Support (i) = 3/5
Intrest rate? = Conf(i -> j) - prob(j) = 3/5 - 4/8

Intrest (dog -> cat) ? - Confi(dog->cat) = 5/7 - 6/8 

## Finding Similar Items 

High Dimensions (X1,...,Xi) we need distance 

Find all pairs of data point (xi,xj) that are d(xi,xj) =< s 

## Jaccard Simialrity 

Sim(C1,C2) = (C1 n C2)/(C1 U C2) 
Distance = 1- Sim(C1,C2) 

## Finding Similar Items 

3 Steps required:
1. Shingling - Convert to sets
2. Min Hashing - Convert to it's Signature
3. Locality Hashing - Compare pair of signature 

## K Shingles 

D1 = abcab -> (ab) (bc) (ca) 

Represent a document by the set of map values of its k-shingles to reduce. We reduce hash value h1(D1) = {1,5,7}

K should be 5 for smaller documents, 10 for the large documents 

Min Hash -> Encoding sets are (0/1) 

D1= 10111, D2 = 10011 What is the Jaccard Similarity? 3/4

## Min Hashing 
![image](https://user-images.githubusercontent.com/79100627/195681963-1ba91cac-0c0a-4af5-8972-c33dde3ca107.png)

If Sim(C1,C2) = H(C1,C2) likely two documents are same documents 
If Sim(C1,C2) != H(C1,C2) likely two documents are not same documents 

We need Random Permutation Pi

![image](https://user-images.githubusercontent.com/79100627/195682225-cdca8507-150c-4a4e-94b7-78a5c3f9c896.png)

Let's Compute the signature 

```
D1 D2 D3 D4
2  1  2  1
2  1  4  1 
1  2  1  2

D1 = 2 2 1 
D2 = 1 1 2 
D3 = 2 4 1 
D4 = 1 1 2 

Sim (D1,D2) = 0
Sim (D1,D3) = 2/3
Sim (D1,D4) = 0 
Sim (D2,D3) = 0 
Sim (D2,D4) = 1
Sim (D3,D4) = 0 

Actual Column 
Sim (D1,D2) = 0 
Sim (D1,D3) = 3/4
Sim (D1,D4) = 0 
Sim (D2,D3) = 0 
Sim (D2,D4) = 3/4
Sim (D3,D4) = 0
```

## Locality Sensitivity Hashing 
- Find the document with Jaccard Sim at least s 
- Arrange similar colums hash into same bucket 

Choose 20 Bands and 5 R whose s>= 0.8 

Probability C1,C2 is identical in one particular band </br>
(0.8)^5</br>
Probability C1,C2 is not similar in 20 bands</br> 
(1-(0.8)^5)^20 </br>

## Formula for Locality Sensitivity Hashing

Columns C1 and C2 have similarity t 

### Probability that all rows in band are equal 

t^r

### Probability that some row in band are unequal 

1-t^r

### Probability that no bands Identical 

(1-(t^r)^b

### Probability that at least one bands Identical

(1-(1-(t^r))^b)

## Exercises 

![image](https://user-images.githubusercontent.com/79100627/195688833-b7969e44-7e42-475a-a60e-9f81de21b5be.png)

What is the Jaccard Similarity of this?

3/7 

## Clustering

Set of point, with a notion of distance between points, group of the points into some number of clusters, so that 
- members of a cluster are close/similar to each other 
- member of a different clusters are dissimilar 

Why is it hard? 
-> Clustering in two dimension looks easy 
-> Clustering small amounts of data looks easy 
-> If it goes bigger data set it will be harsh 

## Method of Distances 

Set as Sets -> Jaccard Distances 
Set as Vectors -> Cosine Distinace 
Set as Points -> Euclidean Distance 

## Euclidean Distance 

d(q,p) = sqrt((q1-p1)^2+...+(qi - pi)^2)

## Hierarchical Clustering 

Agglomerative 
- Each point of cluster
- Repeatedly combine two nearest clusters 
- We have to calculate the centroid 

## Non Euclidean Cases

- There is no average between two points 
- Clustroid -> "Cloestest" to other points 
  - Treat Clustroid as if it were centroid 
- Nearness of Clusters 
1. Treat Clusteroid as if it were centroid 
2. Minimum of the distance between any two points 
3. Pick a notion of "cohesion" of clusters (Maximum distance between any two points, one from each clusters) 

## K mean clustering 

- Assume Euclidean Space Distance 
- Start by picking k, the number of cluster
- Initialize cluster by picking one point cluster 

## Populating Clusters 

1. For each point, place it in the cluster whose current centroid it is nearest 
2. After all points, are assigned, update the locations of centroids of the K Clusters 
3. Reassign all points to their closest centroid 

## How to select K?

1. Try different k, looking at the change in the average distance to centroid as K increases 
2. Average fall rapidly until right k, then changes little 

Let's Say we have following points:

2,3,5,8,15.21,25,29

Picked three random points 3,5,15

### Pass1

3:2 <br/>
5:8 <br/>
15:21,25,29 <br/>

### Pass2 

3:2,5
7:8,15
23:21,25,29

### Pass3 

3:2,5
12: 8,15
25: 21,29

## BFR Algorithm - Normally Distributed 

A variant of K-means desgined to handle very large data set O(Clusters)

- Points are read from disk into main-memory
- Most points from previous memory loads are summarized by simple statistic 
- To begin, from the initial load, we select the initial K centroid by some sensible approach 
  - Take K random point
  - Take Small Random Sample and Cluster optionally 
  - Take a sample i pick a random point and then K-1 more points 
  
  BFR Stores Following
  ```
  N - Number of Points
  SUM - Sum of coordinates
  SUMSQ - Sum of Squares of coordinates
  Centroid - Centorid 
  Var - Variance of coordinates 
  
  N = (3,5) (2,1) 
  SUM = (3+2,5+1)
  SUMSQ = (3^2+ 2^2, 5^2 + 1^2) = (13,26) 
  Centroid = (3+2/2, 5+1/2) = (2.5,3) 
  Var = SUMSQ/n - (SUM/n)^2 for each x and y (13/2)-(5/2)^2 (first Var) (26/2) - (6/2)^2 (second Var) 

  ```
  
## Memory Load of Points

1. Find those points that are sufficiently close to a cluster centroid and add those points to that cluster and the DS
2. Use any main-memory clustering algorithm to cluster the remaining points and the old RS 
- Clusters go to the CS; outlying points to the RS 
3. DS set: Adjust statistics of the clusters to account for the new points 
- Add Ns, SUMs, SUMSQs 
4. Consider merging compressed sets in the CS 
5. If this is the last round (last chunck of data), merge all compressed sets in the CS and all RS points into their nearest cluster (or treat them as outliers)

## Mahlanobis Distance 

Point (X1,...Xi) and Centroid (C1,...Cd) 

d(x,c) = sqrt((xi-ci)^2/standardDIv(sqrt(var)))

Combine, if the combined variance is below some threshold  

- Accept a point for a cluster if its M.D. is < some threshold, e.g. 2 standard deviations 

### Should 2 CS subclusters be combined? 
- Compute the variance of the combined subcluster 
  - N, SUM, and SUMSQ allow us to make that claculation quickly 
- Combine if the combined variance is below some threshold 

## CURE Algorithm 

Problem with BFR:
- Assumes clusters are normally distributed in each dimension 
- Axes are fixed - ellipses at an angle are not OK 

- Assumes a Eclidean DIstance 
- Allows clusters to assume any shape 
- Uses a collection of representative points to represent clusters 

### 2 Pass Algorithm 

## Pass 1 
1. Pick random sample points that fit in main memory 
2. Initial Clusters 
- Cluster these points hierarchically - group nearest points/clusters 
3. Pick representative points: 
- For each cluster, pick a sample of points, as dispersed as possible 
- For the sample, pick representatives by moving them 20% toward the centroid of the cluster

Pass 2
Now, rescan the whole dataset and visit each point p in the data set 

Place it in the closest cluster 
- Normal definition of closest find the cloest representative to p and assign it to representative's cluster 


