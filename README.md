# Big-Data

Big Data Studies Note

## Frequent Itemset Mining & Association Rules 

Items - elements in basket 
Basket - small subset of items (transactions) 

![image](https://user-images.githubusercontent.com/79100627/195438680-a485938c-db34-444d-b799-fa0d6c0d6a63.png)

{milk} -> {coke}: If someone buy milk, he is most likely buy a coke

### Example of Baskets and Items 

Basket: Sentence Items: Words 
Basket: Patient Items: Side Effects 

Support for Itemset I: Number of baskets containing all items in I. 
(Frequent Itemset) Support Threshold: Appear at least s baskets (Transactions) 

Assoication Rule: {i1,i2,i3 ... ik} -> j

if basket contains i1 ~ ik then it is likely to contain j

## Confidence 

Confidence (i -> j) = Support(I U J) / Support (I)

*Not all the high confidence rules are interesting. For example, if everyone bought coke this might be useless information

## Interest 

Interest (i -> j) = Confidence( i -> j) - Prob[j]

## Example 1

![image](https://user-images.githubusercontent.com/79100627/195440190-51a73a97-0a2e-4f01-8e9a-dbba6274ff0a.png)

What is the confidence of {m,b} -> c?

Support for {m,b,c} = 2 
Support for {m,b} = 4

Since confidence = Support (i U j) / support(i) = 2/4 = 1/2

What could be the interst?

Conf(i - j) = 1/2 and probability of {c} is 5/8. 
Int(i-> j) = 1/2 - 5/8  

If {i1,i2,...,ik} -> j has high support and high confidence then {i1,i2,...ik} and {i1,i2,...ik} also "frequent" 

## Mining Assoication Rule 

Step 1. Find all frequent itemset I.
Step 2. Rule generation : If I is frequent, A is also frequent 

![image](https://user-images.githubusercontent.com/79100627/195441070-42653621-1a8f-4810-af49-777ef4fe8c58.png)

Items sets that are above the threshold <br/>
Items: m, c, b, j <br/>
Pair item candidate N*(N-1)/2 = {m,c}, {m,b}, {m,j}, {c,b}, {c,j}, {b,j}<br/>
Pair item counts = {m,c,3}, {m,b,4}, {m,j,2}, {c,b,5}, {c,j,3}, {b,j,2}<br/>
Confidence of frequent pairs <br/>
Conf(m->c) = Support(m U c) / support(m) = 3/5<br/>
Conf(m->b) = Support(m U b) / support(m) = 4/5<br/>
Conf(m->j) = Support(m U j) / support(m) = 2/5<br/>
Conf(c->b) = Support (c U b) / support(c) = 5/5 <br/>
Conf(c->j) = Support (c U j) / support(c) = 3/5<br/>
Conf(b->j) = Support (b U j) / support(b) = 2/6<br/>

overall, m->b, m->c, b->c, c->j 

## Maximal Frequent Itemset -> s = 3 (Only count the Superset) 

A - 4  (NO)<br/>
B - 5 (NO)<br/>
C - 3 (NO)<br/>
AB - 4 (Yes)<br/>
AC - 2 x<br/>
BC - 3 (Yes)<br/>
ABC - 2 x<br/>


## Counting Pairs
The Hardest part is **Computing Pairs**.

There are two approahces counting Pairs 

### Approach 1: 4 bytes Pairs  (Matrix)

![image](https://user-images.githubusercontent.com/79100627/195444181-05f9afed-b284-444d-ae64-3e7b5de9b67d.png)

### Approach 2: 12 bytes Pairs (Triples) (Only Count c > 0) 

![image](https://user-images.githubusercontent.com/79100627/195444297-93525458-1168-4e31-a573-27980fb0a6d1.png)

Approach 1 < Approach 2 iff less than 1/3 pairs occur) 

## Apriori Algorithm k pass 

Key Idea is monoticity 

If a set of items i appears at least s times, so does every subset of i 

![image](https://user-images.githubusercontent.com/79100627/195444727-200896bf-7103-4db9-81bb-86fe1d131b2f.png)

![image](https://user-images.githubusercontent.com/79100627/195444776-6e4435e4-6edd-49fb-8cb5-3f222b37b14a.png)

*C3 is generated by L1 & C2 (make candidates of triples with l1 and c2) 

## PCY algorithm (Park-Chen-Yu) Algorithm

In Apriori algorithm, during the pass1, most memory is idle
- Count the individual item 
- each pair of item, hash the pari to the bucket 

Pass2 
- Only count pairs that hashed to the bucket. 

![image](https://user-images.githubusercontent.com/79100627/195446313-312d76a5-d9ea-4150-91e4-a30ccd3059d1.png)

Another approach is to replace buckets by a bit-vector:
- 1 means the bucket count exceeded the support s (call it a frequent bucket); 0 means it did not 

## Multistage Algorithm 

Limit the number of candidate to be counted 
- For Big amount of data, the Main memory is the bottleneck 
- Still need to generate all itemsets but we only want to count/ keep track of that are frequent 

After Pass 1 of PCY, rehash only those pairs that qualify for pass 2 

Require three passes 

## Multistage vs Multihash 

Multistage 
- 3 Passes
- Bit-Vector eventually consume all main memory 

Multihash 
- Use Several Hashtables on first stage 
- Having number of buckets doubles average count 
- 2 Passes 

## Frequent Itemset in <= 2 passes

- Random Sampling
  - Take Random sample (Portion of Basket) 
  - Smaller Threshold 
  - Itemset become a candidate if it is found to be frequent 
- SON 
  - distributed data mining 
  - Compute freuqent item for each node 
- Toivonen 

